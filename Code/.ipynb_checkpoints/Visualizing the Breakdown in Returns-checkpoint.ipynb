{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "code_show=true; \n",
       "function code_toggle() {\n",
       " if (code_show){\n",
       " $('div.input').hide();\n",
       " } else {\n",
       " $('div.input').show();\n",
       " }\n",
       " code_show = !code_show\n",
       "} \n",
       "$( document ).ready(code_toggle);\n",
       "</script>\n",
       "The raw code for this IPython notebook is by default hidden for easier reading.\n",
       "To toggle on/off the raw code, click <a href=\"javascript:code_toggle()\">here</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " } else {\n",
    " $('div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "The raw code for this IPython notebook is by default hidden for easier reading.\n",
    "To toggle on/off the raw code, click <a href=\"javascript:code_toggle()\">here</a>.''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing the Breakdown in Systematic Factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since 2008-2010, the returns to a variety of sytematic factors across asset classes have fallen by roughly 75%. I interpret this as evidence that funds are getting better at trading on systematic trading strategies over time. The fall in returns has several implications for asset pricing research:\n",
    "\n",
    "1. It changes the facts that cross sectional asset pricing theories need to answer. Risk based theories only need to explain 30% of the size of historical factor returns, since the remaining 70% was likely due to mispricing.\n",
    "2. Risk factors should be judged not only by their backtests, but by their more recent performance. If investors learn to trade on systematic strategies, then we should expect to see certain anomalies do very well historically but underperform in the more recent period.\n",
    "\n",
    "McLean and Pontiff (2016) show that portfolio returns tend to drop by 60% after publication. My study differs from theirs along three dimensions. \n",
    "\n",
    "1. I'm interested in aggregate time effects, not time relative to publication. Two of the anomalies I consider (value, momentum) were published in the early 1990's, yet the strategies did very well up until 2008. A third anomaly (time series momentum) was first published in 2011, but the returns started decaying even before the result was published. The decay that I am looking at seems more closely related with higher overall ability in financial markets to implement systematic strategies, not simply the diffusion of knowledge that these anomalies have historically existed.\n",
    "2. I consider evidence from multiple asset classes. \n",
    "3. I am interested in both cross sectional and time series strategies. I am concerned with the broader class of systematic trading strategies, and not just cross sectional predictors.\n",
    "\n",
    "The interactive chart below shows the cumulative returns to 3 kinds of strategies (value, cross sectional momentum, and time series momentum) across four asset classes (equities, commodities, bonds, and currencies). The cumulative returns are indexed to Jan 2008, and are scaled so that the average log return prior to Jan 2008 is 10% annualized. Thus higher Sharpe ratio strategies have smoother lines. Prior to 2008, all the lines trend up, indicating high Sharpe ratios in the past. After 2008, all the lines move sideways, indicating that the post crisis Sharpe ratios are much lower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas.tseries.offsets import MonthEnd\n",
    "import numpy as np\n",
    "from beakerx import *\n",
    "from beakerx.object import beakerx\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sci\n",
    "from copy import copy\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "CUTOFF_DATE = pd.to_datetime('2008-01-01', format = '%Y-%m-%d') + MonthEnd(0)\n",
    "START_DATE = '1970-01-01'\n",
    "NORM_LEVEL = 0.10 / 12\n",
    "AGG_LEVEL = ['Asset Class', 'Descriptor']\n",
    "\n",
    "# Import data\n",
    "factors_raw = read_hdf('../Data/data.h5', key = 'all_factors').reset_index()\n",
    "\n",
    "# Average the factor returns by strategy type\n",
    "factors_avg = factors_raw.groupby(['Asset Class', 'Descriptor', 'Month_date'])['Month_ret'].mean().reset_index()\n",
    "\n",
    "# Normalize the pre-2010 returns to the same level\n",
    "factors = copy(factors_avg)\n",
    "factors = factors.loc[factors['Month_date'] >= START_DATE, ]\n",
    "factors['log_ret'] = np.log(factors['Month_ret'] + 1)\n",
    "factors['Norm_ret'] = factors.groupby(by = AGG_LEVEL, group_keys = False).apply(lambda g: g.log_ret * NORM_LEVEL / g.loc[g['Month_date'] < CUTOFF_DATE, 'log_ret'].mean())\n",
    "factors['Cum_ret'] = factors.groupby(by = AGG_LEVEL, group_keys = False).apply(lambda g: g['Norm_ret'].cumsum())\n",
    "factors['Norm_cum_ret'] = factors.groupby(by = AGG_LEVEL, group_keys = False).apply(lambda g: g['Cum_ret'] - g.loc[g['Month_date'] == CUTOFF_DATE, 'Cum_ret'].values[0])\n",
    "\n",
    "# Ok plot just the equity plots Ax the countries\n",
    "factors = factors.set_index(AGG_LEVEL)\n",
    "all_types = factors.index.unique().values\n",
    "\n",
    "cum_ret_plots = TimePlot(title = 'Cumulative Returns of Strategies', legendLayout=LegendLayout.HORIZONTAL,\\\n",
    "                          legendPosition=LegendPosition(position=LegendPosition.Position.RIGHT),\\\n",
    "                        initWidth = 1000,\\\n",
    "                        initHeight = 1000)\n",
    "\n",
    "BANNED_STRATEGIES = ['Cmd_Value', 'Equities_Quality']\n",
    "\n",
    "for x in all_types:\n",
    "    string_name = '_'.join(x)\n",
    "    if string_name not in BANNED_STRATEGIES:\n",
    "        cum_ret_plots.add(Line(displayName = '_'.join(x), \\\n",
    "                               x = factors.loc[x, 'Month_date'],\\\n",
    "                               y = factors.loc[x, 'Norm_cum_ret']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e68781c9d9445e88a33bcbed5f8d6fc",
       "version_major": 2,
       "version_minor": 0
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cum_ret_plots.setYBound(-5, 2.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Formal Statistical Test\n",
    "\n",
    "Suppose the null hypothesis is that there is no structural break. Suppose that return errors are essentially uncorrelated across time but correlated across strategies. Thus\n",
    "\n",
    "$$ R_t \\sim N(\\mu, \\Sigma) \\mbox{ iid Across Time}$$\n",
    "\n",
    "Suppose that I take the sample mean in the first T periods and then another sample mean in the last S periods. We then have that the distribution of this difference in sample means is\n",
    "\n",
    "$$ T^{-1} \\Sigma_{t=1}^{t=T} R_t - S^{-1} \\Sigma_{t=T+1}^{t=T+S} R_t \\sim N\\left(0, (T^{-1} + S^{-1}) \\times \\Sigma \\right)$$\n",
    "\n",
    "We can use any consistent estimator of this variance covariance matrix, so we can just take the full sample estimate. The chart below shows the p value of this test at different split dates. The test does not detect a structural break until somewhere in the 2003-2010 region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe9eb5ff7ed84bdf8cdbffb58abf4965",
       "version_major": 2,
       "version_minor": 0
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wide_returns = factors[['Month_date', 'log_ret']].reset_index()\n",
    "wide_returns['strategy'] = [x + '_' + y for x,y in zip(wide_returns['Asset Class'], wide_returns['Descriptor'])]\n",
    "wide_returns = wide_returns.loc[[x not in BANNED_STRATEGIES for x in wide_returns['strategy']]]\n",
    "wide_returns = wide_returns[['Month_date', 'strategy', 'log_ret']].pivot('Month_date', 'strategy', 'log_ret')\n",
    "\n",
    "Sigma = wide_returns.cov().values\n",
    "\n",
    "def calc_test_stat(wide_returns, Sigma, split_date):\n",
    "    first_data = wide_returns[wide_returns.index < split_date]\n",
    "    second_data = wide_returns[wide_returns.index > split_date]\n",
    "    \n",
    "    CONSERVATIVE_T = first_data.dropna().shape[0]\n",
    "    T = first_data.shape[0] * 0.5 + CONSERVATIVE_T * 0.5\n",
    "    # T = CONSERVATIVE_T\n",
    "    S = second_data.shape[0]\n",
    "    p = first_data.shape[1]\n",
    "    \n",
    "    if T * S == 0:\n",
    "        return (np.NaN, np.NaN)\n",
    "    \n",
    "    x_bar_first = first_data.mean()\n",
    "    x_bar_second = second_data.mean()\n",
    "    \n",
    "    return_diff = x_bar_second.values - x_bar_first.values\n",
    "    return_diff = return_diff.reshape(-1, 1)\n",
    "    \n",
    "    multiplier = (T ** -1 + S ** -1) ** -1\n",
    "    test_stat = multiplier * np.matmul(np.matmul(return_diff.T, np.linalg.inv(Sigma)), return_diff)\n",
    "    \n",
    "    \n",
    "    # A more robust estimator\n",
    "    variances = np.diag(Sigma)\n",
    "    standard_deviations = np.sqrt(variances)\n",
    "\n",
    "    init_sharpe_ratios = np.divide(x_bar_first.values + variances / 2, standard_deviations)\n",
    "    later_sharpe_ratios = np.divide(x_bar_second.values + variances / 2, standard_deviations)\n",
    "    ret_degrade = later_sharpe_ratios.mean() / init_sharpe_ratios.mean()\n",
    "    \n",
    "    if np.isnan(test_stat[0]):\n",
    "        return (np.nan, np.nan)\n",
    "    \n",
    "    return (float(sci.stats.chi2.sf(test_stat, p)), ret_degrade, test_stat)\n",
    "\n",
    "test_stat_split_by_date = lambda d: calc_test_stat(wide_returns, Sigma, d)[0]\n",
    "ret_diff_split_by_date = lambda d: calc_test_stat(wide_returns, Sigma, d)[1]\n",
    "\n",
    "p_seq = [test_stat_split_by_date(x) for x in wide_returns.index]\n",
    "ret_seq = [ret_diff_split_by_date(x) for x in wide_returns.index]\n",
    "result_frame = pd.DataFrame.from_dict({'Month_date': wide_returns.index, 'P Values': p_seq, 'SR Degradation': ret_seq})\n",
    "\n",
    "result_frame = result_frame.loc[result_frame['Month_date'].between('1990-01-31', '2011-12-31')]\n",
    "\n",
    "def simple_plot(dataframe, variable, plot_title, series_title):\n",
    "    plot = TimePlot(title = plot_title, legendLayout=LegendLayout.HORIZONTAL,\\\n",
    "                          legendPosition=LegendPosition(position=LegendPosition.Position.RIGHT),\\\n",
    "                        initWidth = 600,\\\n",
    "                        initHeight = 300)\n",
    "    \n",
    "    plot.add(Line(displayName = series_title, \\\n",
    "                       x = dataframe['Month_date'],\\\n",
    "                       y = dataframe[variable]))\n",
    "    plot.setYBound(0, 1)\n",
    "    return plot\n",
    "    \n",
    "simple_plot(result_frame, 'P Values', 'P Value of Test of Structural Break by Split Date', 'P Value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to visualize the breakdown in returns is to take the average Sharpe ratio in the \"post\" period across these strategies and divide it by the average Sharpe ratio in the \"pre\" period, splitting by various dates. This suggests that Sharpe ratios have fallen by around 75% (i.e. they are 25% of what they used to be) in the post 2008 period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "468859a8e6cb4dcc88208273d86d9844",
       "version_major": 2,
       "version_minor": 0
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "simple_plot(result_frame, 'SR Degradation', 'Reduction in Average Sharpe Ratios by Split Date', 'Ratio')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Work\n",
    "\n",
    "1. Incorporate more factors?\n",
    "2. Take out the unconditional loading on overall market risk factors such as bonds or equities. For example, time series momentum on bonds does really well in the post 2008 periods because it's been a 10 year bull market in bonds. Therefore the results above likely overstate the returns to some of these strategies.\n",
    "3. Investigate the construction of some of these series? I think they're all flawed because they all use some variant on equal weighting -- i.e. that they require the fund to have large stakes in small companies. How would these series look if we did a different kind of replication?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
