{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas.tseries.offsets import MonthEnd\n",
    "import numpy as np\n",
    "from beakerx import *\n",
    "from beakerx.object import beakerx\n",
    "import scipy.stats as stats\n",
    "from copy import copy\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "code_show=true; \n",
       "function code_toggle() {\n",
       " if (code_show){\n",
       " $('div.input').hide();\n",
       " } else {\n",
       " $('div.input').show();\n",
       " }\n",
       " code_show = !code_show\n",
       "} \n",
       "$( document ).ready(code_toggle);\n",
       "</script>\n",
       "The raw code for this IPython notebook is by default hidden for easier reading.\n",
       "To toggle on/off the raw code, click <a href=\"javascript:code_toggle()\">here</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " } else {\n",
    " $('div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "The raw code for this IPython notebook is by default hidden for easier reading.\n",
    "To toggle on/off the raw code, click <a href=\"javascript:code_toggle()\">here</a>.''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Sample of Systematic Trading Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this file is to provide an easy way to visualize the returns to a variety of systematic trading strategies, across asset classes and across countries. Factors include:\n",
    "\n",
    "1. Classic cross sectional equity factors: value, momentum, betting against beta and quality. \n",
    "2. Value, momentum, and betting against beta in other asset classes. \n",
    "3. Time series momentum, which differs from the cross sectional strategies above in that it is not market neutral at all points in time.\n",
    "\n",
    "Two facts jump out to me when I look at these charts.\n",
    "\n",
    "1. Factor strategies no longer earn the returns they used to. Since 2008, the returns to a large set of \"robust\" textbook factors that span asset classes and countries have gone down by roughly 50%. Since these factors were well known, robust factors, I interpret the decay in returns as evidence that funds are getting better at trading on systematic trading strategies over time and that to the extent these factors represented risk factors, these risks are becoming more widely held. \n",
    "2. There's no way that these factors represent \"tradable returns\" accessible to an investor. A diversified basket of all the equity factors across all the countries earns a Sharpe ratio of 1.6, while being uncorrelated to the broader market. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "CUTOFF_DATE = pd.to_datetime('2008-01-01', format = '%Y-%m-%d') + MonthEnd(0)\n",
    "START_DATE = '1930-01-01'\n",
    "NORM_AVG = 0.10 / 12\n",
    "NORM_STD = 0.10 / np.sqrt(12)\n",
    "ALL_COUNTRIES = ['USA', 'WLD', 'GBR', 'JPN', 'DEU', 'FRA', 'ITA', 'CAN', 'AUS']\n",
    "ALL_CTRY_EX_WORLD = copy(ALL_COUNTRIES)\n",
    "ALL_CTRY_EX_WORLD.remove('WLD')\n",
    "ALL_ASSETS = ['Equities', 'FX', 'Bonds', 'Cmd']\n",
    "\n",
    "# Import data\n",
    "factors_raw = read_hdf('../Data/data.h5', key = 'all_factors')\n",
    "\n",
    "def normalize_factors(factor_dataset, normalization_var, aggregation_level, country_list, cutoff_date = CUTOFF_DATE, start_date = START_DATE):\n",
    "    factors = factor_dataset.loc[factor_dataset.index.get_level_values('Country').isin(country_list), :]\n",
    "    factors = factors.groupby(aggregation_level + ['Month_date']).mean()\n",
    "    factors = factors.reset_index().set_index(aggregation_level)\n",
    "    factors = factors.loc[factors['Month_date'] >= start_date, :]\n",
    "    \n",
    "    # Build return variables\n",
    "    factors['Log Return'] = np.log(factors['Month_ret'] + 1)\n",
    "    factors['Before Cutoff'] = (factors['Month_date'] < cutoff_date)\n",
    "    factors = factors.reset_index().set_index(aggregation_level)\n",
    "    if normalization_var == 'Avg':\n",
    "        factors['Avg Return Before Cutoff'] = factors.groupby(by = aggregation_level).apply(lambda d: d.loc[d['Before Cutoff'], 'Log Return'].mean())\n",
    "        factors['Normalized Log Return'] = factors['Log Return'] * NORM_AVG / factors['Avg Return Before Cutoff']\n",
    "    elif normalization_var == 'Vol':\n",
    "        factors['Avg Vol Before Cutoff'] = factors.groupby(by = aggregation_level).apply(lambda d: d.loc[d['Before Cutoff'], 'Log Return'].std())\n",
    "        factors['Normalized Log Return'] = factors['Log Return'] * NORM_STD / factors['Avg Vol Before Cutoff']\n",
    "    else:\n",
    "        assert(0)\n",
    "        \n",
    "    factors['Normalized Cumulative Return'] = factors.groupby(by = aggregation_level)['Normalized Log Return'].cumsum()\n",
    "    factors['Cumulative Return on Cutoff'] = factors.groupby(by = aggregation_level).apply(lambda d: d.loc[d['Month_date'] == cutoff_date, 'Normalized Cumulative Return'].values[0])\n",
    "    factors['Normalized Cumulative Return'] = factors['Normalized Cumulative Return'] - factors['Cumulative Return on Cutoff']\n",
    "    return factors\n",
    "\n",
    "# CMD Value looks particularly bad\n",
    "factors_to_use = factors_raw\n",
    "factors_to_use = factors_to_use.loc[~((factors_to_use.index.get_level_values('Asset Class') == 'Cmd') &\\\n",
    "                                      (factors_to_use.index.get_level_values('Descriptor') == 'Value')), :]\n",
    "factors_to_use = factors_to_use.reset_index().set_index(['Descriptor', 'Asset Class', 'Country'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strategies Averaged Across Countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_plot_for_frame(factor_dataset, plot_title = 'Cumulative Returns', **kwargs):\n",
    "\n",
    "    all_types = factor_dataset.index.unique().values\n",
    "    cum_ret_plots = TimePlot(title = plot_title, legendLayout=LegendLayout.HORIZONTAL,\\\n",
    "                              legendPosition=LegendPosition(position=LegendPosition.Position.TOP), **kwargs)\n",
    "\n",
    "    for x in all_types:\n",
    "        if type(x).__name__ == 'str':\n",
    "            string_name = x\n",
    "        else:\n",
    "            string_name = ' '.join(x)            \n",
    "        cum_ret_plots.add(Line(displayName = string_name, \\\n",
    "                               x = factor_dataset.loc[x, 'Month_date'],\\\n",
    "                               y = factor_dataset.loc[x, 'Normalized Cumulative Return']))\n",
    "    return cum_ret_plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "asset_class_by_strategy = normalize_factors(factors_to_use, 'Avg', ['Asset Class', 'Descriptor'], ALL_COUNTRIES, start_date = '1960-01-01')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = asset_class_by_strategy.groupby(['Asset Class', 'Descriptor'])['Month_ret'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2227733bc9e64d3a8e30f6822e5bef41",
       "version_major": 2,
       "version_minor": 0
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea5febc0437947c59cca69beb629828f",
       "version_major": 2,
       "version_minor": 0
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "build_plot_for_frame(asset_class_by_strategy, plot_title = 'Cumulative Returns (ln, Normalized to 10% Avg Return)', initWidth = 1000, initHeight = 800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Country Level Deep Dive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_equities = normalize_factors(factors_to_use, 'Vol', ['Asset Class', 'Descriptor', 'Country'], ALL_CTRY_EX_WORLD, start_date = '1988-01-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctry_plots = []\n",
    "for c in ALL_CTRY_EX_WORLD:\n",
    "    ctry_plots.append(build_plot_for_frame(country_equities.loc[country_equities.index.get_level_values('Country') == c, :], plot_title = 'Cum. Ret (ln, Norm. to 10% Vol)', initWidth = 500, initHeight = 400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_list(list_of_beakerx_plots):\n",
    "\n",
    "    lg = GridOutputContainerLayoutManager(3)\n",
    "    og = OutputContainer()\n",
    "    og.setLayoutManager(lg)\n",
    "        \n",
    "    for p in list_of_beakerx_plots:\n",
    "        og.addItem(p)\n",
    "    return og"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "039619a573f544da87eb353c729ba061",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GridView(children=(BeakerxHBox(children=(TimePlot(model={'chart_title': 'Cum. Ret (ln, Norm. to 10% Vol)', 'co…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_list(ctry_plots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Returns to different classes of strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns_by_strategy = normalize_factors(factors_to_use, 'Avg', ['Descriptor'], ALL_COUNTRIES, start_date = '1930-01-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53f0e26b08e24a86b8ed95d970dcbd73",
       "version_major": 2,
       "version_minor": 0
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "build_plot_for_frame(returns_by_strategy, plot_title = 'Cumulative Returns (ln, Normalized to 10% Average)', initWidth = 1000, initHeight = 700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Returns to different asset classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d034ee9d61d47d3bf6365fc6cc9f1ce",
       "version_major": 2,
       "version_minor": 0
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "returns_by_asset_class = normalize_factors(factors_to_use, 'Vol', ['Asset Class'], ALL_COUNTRIES, start_date = '1970-01-01')\n",
    "build_plot_for_frame(returns_by_asset_class, plot_title = 'Cumulative Returns (ln, Normalized to 10% Vol)', initWidth = 1000, initHeight = 700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Formal Statistical Test\n",
    "\n",
    "Suppose the null hypothesis is that there is no structural break. Suppose that return errors are essentially uncorrelated across time but correlated across strategies. Thus\n",
    "\n",
    "$$ R_t \\sim N(\\mu, \\Sigma) \\mbox{ iid Across Time}$$\n",
    "\n",
    "Suppose that I take the sample mean in the first T periods and then another sample mean in the last S periods. We then have that the distribution of this difference in sample means is\n",
    "\n",
    "$$ T^{-1} \\Sigma_{t=1}^{t=T} R_t - S^{-1} \\Sigma_{t=T+1}^{t=T+S} R_t \\sim N\\left(0, (T^{-1} + S^{-1}) \\times \\Sigma \\right)$$\n",
    "\n",
    "We can use any consistent estimator of this variance covariance matrix, so we can just take the full sample estimate. The chart below shows the p value of this test at different split dates. The test does not detect a structural break until somewhere in the 2003-2010 region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "wide_returns = asset_class_by_strategy[['Month_date', 'Log Return']].reset_index()\n",
    "wide_returns['strategy'] = [x + '_' + y for x,y in zip(wide_returns['Asset Class'], wide_returns['Descriptor'])]\n",
    "wide_returns = wide_returns[['Month_date', 'strategy', 'Log Return']].pivot('Month_date', 'strategy', 'Log Return')\n",
    "\n",
    "Sigma = wide_returns.cov().values\n",
    "\n",
    "def calc_test_stat(wide_returns, Sigma, split_date):\n",
    "    first_data = wide_returns[wide_returns.index < split_date]\n",
    "    second_data = wide_returns[wide_returns.index > split_date]\n",
    "    \n",
    "    CONSERVATIVE_T = first_data.dropna().shape[0]\n",
    "    T = 0.3 * first_data.shape[0] + 0.7 * CONSERVATIVE_T\n",
    "    S = second_data.shape[0]\n",
    "    p = first_data.shape[1]\n",
    "    \n",
    "    if T * S == 0:\n",
    "        return (np.nan, np.nan, np.nan, np.nan)\n",
    "    \n",
    "    x_bar_first = first_data.mean()\n",
    "    x_bar_second = second_data.mean()\n",
    "    variances = np.diag(Sigma)\n",
    "    standard_deviations = np.sqrt(variances)\n",
    "    jensen = variances / 2\n",
    "    return_precision = np.linalg.inv(Sigma)\n",
    "    \n",
    "    # An optimal degradation measure based on a precisely measured covariance matrix\n",
    "    pre_arith_returns = x_bar_first + variances / 2\n",
    "    post_arith_returns = x_bar_second + variances / 2\n",
    "    pre_optimal_sharpe = np.sqrt(pre_arith_returns.T  @ return_precision @ pre_arith_returns)\n",
    "    post_optimal_sharpe = np.sqrt(post_arith_returns.T @ return_precision @ post_arith_returns)\n",
    "    sr_degrade = post_optimal_sharpe / pre_optimal_sharpe\n",
    "        \n",
    "    # A more robust estimator\n",
    "    init_sharpe_ratios = np.divide(x_bar_first.values + variances / 2, standard_deviations)\n",
    "    later_sharpe_ratios = np.divide(x_bar_second.values + variances / 2, standard_deviations)\n",
    "    ret_degrade = np.median(later_sharpe_ratios / init_sharpe_ratios)\n",
    "    \n",
    "    # Build the test statistic\n",
    "    multiplier = (T ** -1 + S ** -1) ** -1\n",
    "    ret_diff = x_bar_second - x_bar_first\n",
    "    test_stat = multiplier * ret_diff.T @ return_precision @ ret_diff\n",
    "    \n",
    "    if np.isnan(test_stat):\n",
    "        return (np.nan, np.nan, np.nan, np.nan) \n",
    "    \n",
    "    return (float(stats.chi2.sf(test_stat, p)), sr_degrade, ret_degrade, test_stat)\n",
    "\n",
    "test_results = [calc_test_stat(wide_returns, Sigma, x) for x in wide_returns.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c787043a2ca43688b9414b67372f024",
       "version_major": 2,
       "version_minor": 0
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "p_seq = [x[0] for x in test_results]\n",
    "sr_seq = [x[1] for x in test_results]\n",
    "avg_sr_seq = [x[2] for x in test_results if len(x) > 0]\n",
    "\n",
    "result_frame = pd.DataFrame.from_dict({'Month_date': wide_returns.index, 'P Values': p_seq, 'Optimal SR Degradation': sr_seq, 'Avg SR Degradation': avg_sr_seq})\n",
    "\n",
    "result_frame = result_frame.loc[result_frame['Month_date'].between('1990-01-31', '2011-12-31')]\n",
    "\n",
    "def simple_plot(dataframe, variable, plot_title, series_title):\n",
    "    plot = TimePlot(title = plot_title, legendLayout=LegendLayout.HORIZONTAL,\\\n",
    "                          legendPosition=LegendPosition(position=LegendPosition.Position.RIGHT),\\\n",
    "                        initWidth = 1000,\\\n",
    "                        initHeight = 700)\n",
    "    \n",
    "    plot.add(Line(displayName = series_title, \\\n",
    "                       x = dataframe['Month_date'],\\\n",
    "                       y = dataframe[variable]))\n",
    "    plot.setYBound(0, 1)\n",
    "    return plot\n",
    "    \n",
    "simple_plot(result_frame, 'P Values', 'P Value of Test of Structural Break by Split Date', 'P Value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to visualize the breakdown in returns is to take the average Sharpe ratio in the \"post\" period across these strategies and divide it by the average Sharpe ratio in the \"pre\" period, splitting by various dates. This suggests that Sharpe ratios have fallen by around 60% (i.e. they are 40% of what they used to be) in the post 2008 period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0de29969837547e89fda7d02b36ffc71",
       "version_major": 2,
       "version_minor": 0
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "simple_plot(result_frame, 'Avg SR Degradation', 'Reduction in Average Sharpe Ratios by Split Date', 'Ratio')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Work\n",
    "\n",
    "1. Incorporate more factors?\n",
    "2. Take out the unconditional loading on overall market risk factors such as bonds or equities. For example, time series momentum on bonds does really well in the post 2008 periods because it's been a 10 year bull market in bonds. Therefore the results above likely overstate the returns to some of these strategies.\n",
    "3. Investigate the construction of some of these series? I think they're all flawed because they all use some variant on equal weighting -- i.e. that they require the fund to have large stakes in small companies. How would these series look if we did a different kind of replication?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
