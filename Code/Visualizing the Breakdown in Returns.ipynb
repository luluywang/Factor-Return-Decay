{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas.tseries.offsets import MonthEnd\n",
    "import numpy as np\n",
    "from beakerx import *\n",
    "from beakerx.object import beakerx\n",
    "import scipy.stats as stats\n",
    "from copy import copy\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " } else {\n",
    " $('div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "The raw code for this IPython notebook is by default hidden for easier reading.\n",
    "To toggle on/off the raw code, click <a href=\"javascript:code_toggle()\">here</a>.''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://mybinder.org/v2/gh/luluywang/Factor-Return-Decay/master?filepath=%2FCode%2FVisualizing%20the%20Breakdown%20in%20Returns.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dude, where is my alpha?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Factor strategies no longer earn the returns they used to. Since 2008, the returns to a large set of \"robust\" textbook factors that span asset classes and countries have gone down by roughly 50%. Since these factors were well known, robust factors, I interpret the decay in returns as evidence that funds are getting better at trading on systematic trading strategies over time and that to the extent these factors represented risk factors, these risks are becoming more widely held. This file should provide an easy way to visualize the returns to these factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "CUTOFF_DATE = pd.to_datetime('2008-01-01', format = '%Y-%m-%d') + MonthEnd(0)\n",
    "START_DATE = '1930-01-01'\n",
    "NORM_AVG = 0.10 / 12\n",
    "NORM_STD = 0.16 / np.sqrt(12)\n",
    "ALL_COUNTRIES = ['USA', 'WLD', 'GBR', 'JPN', 'DEU', 'FRA', 'ITA', 'CAN', 'AUS']\n",
    "ALL_CTRY_EX_WORLD = copy(ALL_COUNTRIES)\n",
    "ALL_CTRY_EX_WORLD.remove('WLD')\n",
    "ALL_ASSETS = ['Equities', 'FX', 'Bonds', 'Cmd']\n",
    "\n",
    "# Import data\n",
    "factors_raw = read_hdf('../Data/data.h5', key = 'all_factors')\n",
    "\n",
    "def normalize_factors(factor_dataset, normalization_var, aggregation_level, country_list, cutoff_date = CUTOFF_DATE, start_date = START_DATE):\n",
    "    factors = factor_dataset.loc[factor_dataset.index.get_level_values('Country').isin(country_list), :]\n",
    "    factors = factors.groupby(aggregation_level + ['Month_date']).mean()\n",
    "    factors = factors.loc[factors.index.get_level_values('Month_date') >= start_date, :]\n",
    "    \n",
    "    # Build return variables\n",
    "    factors['Log Return'] = np.log(factors['Month_ret'] + 1)\n",
    "    factors['Before Cutoff'] = (factors.index.get_level_values('Month_date') < cutoff_date)\n",
    "    \n",
    "    if normalization_var == 'Avg':\n",
    "        factors['Avg Log Return Before Cutoff'] = factors.groupby(by = aggregation_level).apply(lambda d: d.loc[d['Before Cutoff'], 'Log Return'].mean())\n",
    "        factors['Normalized Log Return'] = factors['Log Return'] * NORM_AVG / factors['Avg Log Return Before Cutoff']\n",
    "    elif normalization_var == 'Vol':\n",
    "        factors['Vol of Log Return Before Cutoff'] = factors.groupby(by = aggregation_level).apply(lambda d: d.loc[d['Before Cutoff'], 'Log Return'].std())\n",
    "        factors['Normalized Log Return'] = factors['Log Return'] * NORM_STD / factors['Vol of Log Return Before Cutoff']\n",
    "    else:\n",
    "        assert(0)\n",
    "        \n",
    "    factors['Normalized Cumulative Return'] = factors.groupby(by = aggregation_level)['Normalized Log Return'].cumsum()\n",
    "    factors['Cumulative Return on Cutoff'] = factors.groupby(by = aggregation_level)\\\n",
    "                                             .apply(lambda d: d.loc[d.index.get_level_values('Month_date') == cutoff_date, 'Normalized Cumulative Return'].values[0])\n",
    "    factors['Normalized Cumulative Return'] = factors['Normalized Cumulative Return'] - factors['Cumulative Return on Cutoff']\n",
    "    \n",
    "    \n",
    "    factors = factors.reset_index().set_index(aggregation_level)\n",
    "    return factors\n",
    "\n",
    "# CMD Value looks particularly bad\n",
    "factors_to_use = factors_raw\n",
    "factors_to_use = factors_to_use.loc[~((factors_to_use.index.get_level_values('Asset Class') == 'Cmd') &\\\n",
    "                                      (factors_to_use.index.get_level_values('Descriptor') == 'Value')), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strategies Averaged Across Countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_plot_for_frame(factor_dataset, **kwargs):\n",
    "\n",
    "    all_types = factor_dataset.index.unique().values\n",
    "    cum_ret_plots = TimePlot(title = 'Cumulative Returns of Strategies', legendLayout=LegendLayout.HORIZONTAL,\\\n",
    "                              legendPosition=LegendPosition(position=LegendPosition.Position.TOP), **kwargs)\n",
    "\n",
    "    for x in all_types:\n",
    "        string_name = '_'.join(x)\n",
    "        cum_ret_plots.add(Line(displayName = ' '.join(x), \\\n",
    "                               x = factor_dataset.loc[x, 'Month_date'],\\\n",
    "                               y = factor_dataset.loc[x, 'Normalized Cumulative Return']))\n",
    "    return cum_ret_plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "asset_class_by_strategy = normalize_factors(factors_to_use, 'Avg', ['Asset Class', 'Descriptor'], ALL_COUNTRIES, start_date = '1960-01-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "build_plot_for_frame(asset_class_by_strategy, initWidth = 1000, initHeight = 800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Country Level Deep Dive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_equities = normalize_factors(factors_to_use, 'Vol', ['Asset Class', 'Descriptor', 'Country'], ALL_CTRY_EX_WORLD, start_date = '1988-01-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctry_plots = []\n",
    "for c in ALL_CTRY_EX_WORLD:\n",
    "    ctry_plots.append(build_plot_for_frame(country_equities.loc[country_equities.index.get_level_values('Country') == c, :], initWidth = 500, initHeight = 400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_list(list_of_beakerx_plots):\n",
    "\n",
    "    lg = GridOutputContainerLayoutManager(3)\n",
    "    og = OutputContainer()\n",
    "    og.setLayoutManager(lg)\n",
    "        \n",
    "    for p in list_of_beakerx_plots:\n",
    "        og.addItem(p)\n",
    "    return og"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridView(children=(BeakerxHBox(children=(TimePlot(model={'chart_title': 'Cumulative Returns of Strategies', 'câ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_list(ctry_plots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strategies Across Asset Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns_by_asset_class = normalize_factors(factors_to_use, 'Vol', ['Asset Class'], ALL_COUNTRIES, start_date = '1980-01-01')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Formal Statistical Test\n",
    "\n",
    "Suppose the null hypothesis is that there is no structural break. Suppose that return errors are essentially uncorrelated across time but correlated across strategies. Thus\n",
    "\n",
    "$$ R_t \\sim N(\\mu, \\Sigma) \\mbox{ iid Across Time}$$\n",
    "\n",
    "Suppose that I take the sample mean in the first T periods and then another sample mean in the last S periods. We then have that the distribution of this difference in sample means is\n",
    "\n",
    "$$ T^{-1} \\Sigma_{t=1}^{t=T} R_t - S^{-1} \\Sigma_{t=T+1}^{t=T+S} R_t \\sim N\\left(0, (T^{-1} + S^{-1}) \\times \\Sigma \\right)$$\n",
    "\n",
    "We can use any consistent estimator of this variance covariance matrix, so we can just take the full sample estimate. The chart below shows the p value of this test at different split dates. The test does not detect a structural break until somewhere in the 2003-2010 region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "wide_returns = asset_class_by_strategy[['Month_date', 'Log Return']].reset_index()\n",
    "wide_returns['strategy'] = [x + '_' + y for x,y in zip(wide_returns['Asset Class'], wide_returns['Descriptor'])]\n",
    "wide_returns = wide_returns[['Month_date', 'strategy', 'Log Return']].pivot('Month_date', 'strategy', 'Log Return')\n",
    "\n",
    "Sigma = wide_returns.cov().values\n",
    "\n",
    "def calc_test_stat(wide_returns, Sigma, split_date):\n",
    "    first_data = wide_returns[wide_returns.index < split_date]\n",
    "    second_data = wide_returns[wide_returns.index > split_date]\n",
    "    \n",
    "    CONSERVATIVE_T = first_data.dropna().shape[0]\n",
    "    T = 0.3 * first_data.shape[0] + 0.7 * CONSERVATIVE_T\n",
    "    S = second_data.shape[0]\n",
    "    p = first_data.shape[1]\n",
    "    \n",
    "    if T * S == 0:\n",
    "        return (np.nan, np.nan, np.nan, np.nan)\n",
    "    \n",
    "    x_bar_first = first_data.mean()\n",
    "    x_bar_second = second_data.mean()\n",
    "    variances = np.diag(Sigma)\n",
    "    standard_deviations = np.sqrt(variances)\n",
    "    jensen = variances / 2\n",
    "    return_precision = np.linalg.inv(Sigma)\n",
    "    \n",
    "    # An optimal degradation measure based on a precisely measured covariance matrix\n",
    "    pre_arith_returns = x_bar_first + variances / 2\n",
    "    post_arith_returns = x_bar_second + variances / 2\n",
    "    pre_optimal_sharpe = np.sqrt(pre_arith_returns.T  @ return_precision @ pre_arith_returns)\n",
    "    post_optimal_sharpe = np.sqrt(post_arith_returns.T @ return_precision @ post_arith_returns)\n",
    "    sr_degrade = post_optimal_sharpe / pre_optimal_sharpe\n",
    "        \n",
    "    # A more robust estimator\n",
    "    init_sharpe_ratios = np.divide(x_bar_first.values + variances / 2, standard_deviations)\n",
    "    later_sharpe_ratios = np.divide(x_bar_second.values + variances / 2, standard_deviations)\n",
    "    ret_degrade = later_sharpe_ratios.mean() / init_sharpe_ratios.mean()\n",
    "    \n",
    "    # Build the test statistic\n",
    "    multiplier = (T ** -1 + S ** -1) ** -1\n",
    "    ret_diff = x_bar_second - x_bar_first\n",
    "    test_stat = multiplier * ret_diff.T @ return_precision @ ret_diff\n",
    "    \n",
    "    if np.isnan(test_stat):\n",
    "        return (np.nan, np.nan, np.nan, np.nan) \n",
    "    \n",
    "    return (float(stats.chi2.sf(test_stat, p)), sr_degrade, ret_degrade, test_stat)\n",
    "\n",
    "test_results = [calc_test_stat(wide_returns, Sigma, x) for x in wide_returns.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_seq = [x[0] for x in test_results]\n",
    "sr_seq = [x[1] for x in test_results]\n",
    "avg_sr_seq = [x[2] for x in test_results if len(x) > 0]\n",
    "\n",
    "result_frame = pd.DataFrame.from_dict({'Month_date': wide_returns.index, 'P Values': p_seq, 'Optimal SR Degradation': sr_seq, 'Avg SR Degradation': avg_sr_seq})\n",
    "\n",
    "result_frame = result_frame.loc[result_frame['Month_date'].between('1990-01-31', '2011-12-31')]\n",
    "\n",
    "def simple_plot(dataframe, variable, plot_title, series_title):\n",
    "    plot = TimePlot(title = plot_title, legendLayout=LegendLayout.HORIZONTAL,\\\n",
    "                          legendPosition=LegendPosition(position=LegendPosition.Position.RIGHT),\\\n",
    "                        initWidth = 600,\\\n",
    "                        initHeight = 300)\n",
    "    \n",
    "    plot.add(Line(displayName = series_title, \\\n",
    "                       x = dataframe['Month_date'],\\\n",
    "                       y = dataframe[variable]))\n",
    "    plot.setYBound(0, 1)\n",
    "    return plot\n",
    "    \n",
    "simple_plot(result_frame, 'P Values', 'P Value of Test of Structural Break by Split Date', 'P Value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to visualize the breakdown in returns is to take the average Sharpe ratio in the \"post\" period across these strategies and divide it by the average Sharpe ratio in the \"pre\" period, splitting by various dates. This suggests that Sharpe ratios have fallen by around 75% (i.e. they are 25% of what they used to be) in the post 2008 period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_plot(result_frame, 'Avg SR Degradation', 'Reduction in Average Sharpe Ratios by Split Date', 'Ratio')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Work\n",
    "\n",
    "1. Incorporate more factors?\n",
    "2. Take out the unconditional loading on overall market risk factors such as bonds or equities. For example, time series momentum on bonds does really well in the post 2008 periods because it's been a 10 year bull market in bonds. Therefore the results above likely overstate the returns to some of these strategies.\n",
    "3. Investigate the construction of some of these series? I think they're all flawed because they all use some variant on equal weighting -- i.e. that they require the fund to have large stakes in small companies. How would these series look if we did a different kind of replication?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1.0,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
